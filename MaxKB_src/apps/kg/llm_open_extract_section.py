# docker exec -it maxkb-dev bash -lc '
# export OPENAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4
# export OPENAI_API_KEY=81d9a02a6a2c00c1e543e4eec620446f.1GB9GwtCMpeJMZuZ
# export OPENAI_MODEL=glm-4-flash
# python -m apps.kg.llm_open_extract_section \
#   --glob "/opt/maxkb/data/kg/md/*.md" \
#   --uri "bolt://neo4j:7687" \
#   --user neo4j --password "neo4j_pass123"
# '


import os, re, glob, json, time
from typing import List, Dict
from neo4j import GraphDatabase
from openai import OpenAI
from tqdm import tqdm
from collections import defaultdict

# æ ‡å¿—è¡Œæ­£åˆ™ï¼šæ”¯æŒè¡Œé¦– 'â€”â€”'/'â€”'/'-'/'â€¢'/'Â·'/'ä¸€' å‰ç¼€ï¼›ä¸­æ–‡/è‹±æ–‡æ‹¬å·ï¼›å…¨è§’/åŠè§’å†’å·
FLAG_LINE = re.compile(
    r'^\s*(?:[â€”\-â€¢Â·ä¸€]*\s*)?(?P<flag>[A-Z][A-Z0-9]{1,15})'
    r'(?:[ï¼ˆ(](?P<alias>[^ï¼‰)]{0,50})[ï¼‰)])?\s*[:ï¼š]\s*(?P<body>.+?)\s*[ï¼›;ã€‚]?\s*$'
)

def should_create_entity_node(sec_id: str, title: str, child_count: int, content: str = "") -> bool:
    """
    ç»¼åˆåˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ºç« èŠ‚æ ‡é¢˜åˆ›å»º Entity èŠ‚ç‚¹
    """
    # 1. åŸºç¡€è¿‡æ»¤ï¼ˆä¿æŒä¸å˜ï¼‰
    if not title or len(title.strip()) < 2:
        return False
    if re.search(r'(è¡¨|ç¤ºä¾‹|æ³¨|å›¾|é™„å½•|å‚è€ƒ|bibliography|æ ¼å¼|ç»„æˆ|ç»“æ„|å®šä¹‰|æ¦‚è¿°|èŒƒå›´)', title, re.IGNORECASE):
        return False  # ğŸ‘ˆ æ–°å¢ï¼šæ˜ç¡®æ’é™¤"æ ¼å¼"ã€"ç»„æˆ"ç­‰æ–‡æ¡£ç»“æ„è¯
    depth = len(sec_id.split('.'))
    if depth >= 4:
        return False
        
    # 2. æŠ€æœ¯å…³é”®è¯åŒ¹é…ï¼ˆä¼˜åŒ–å…³é”®è¯é›†ï¼‰
    TECH_CONCEPT_WORDS = {'æ¥å£', 'åè®®', 'åŒæ­¥', 'å¼‚æ­¥', 'ç‰©ç†', 'ä¼ è¾“', 'ç¼–ç ', 
                         'æ ¡éªŒ', 'é€Ÿç‡', 'å¸§', 'ç”µè·¯', 'ç½‘ç»œ', 'åœ°å€', 'æ ‡å¿—', 
                         'ç±»å‹', 'æ ‡å‡†', 'è¦æ±‚', 'è§„èŒƒ', 'å‚æ•°', 'å±æ€§', 'ç”µæŠ¥'}
    
    STRUCTURE_WORDS = {'æ ¼å¼', 'ç»„æˆ', 'ç»“æ„', 'å®šä¹‰', 'æ¦‚è¿°', 'èŒƒå›´', 'å†…å®¹', 'è¯´æ˜'}
    
    title_words = set(re.findall(r'[\u4e00-\u9fa5]{2,}', title))
    
    # å¿…é¡»åŒ…å«æŠ€æœ¯æ¦‚å¿µè¯ï¼Œä¸”ä¸åŒ…å«æ–‡æ¡£ç»“æ„è¯
    has_tech = bool(title_words & TECH_CONCEPT_WORDS)
    has_struct = bool(title_words & STRUCTURE_WORDS)
    
    if has_tech and not has_struct:
        return True
        
    # 3. ç»“æ„ç‰¹å¾åˆ¤æ–­ï¼ˆä¿æŒä¸å˜ï¼‰
    if child_count >= 2:
        return True
        
    # 4. å†…å®¹ç‰¹å¾åˆ¤æ–­ï¼ˆä¿æŒä¸å˜ï¼‰
    if content and len(content) < 800 and re.search(r'åŒ…æ‹¬|åŒ…å«|å¦‚ä¸‹', content):
        return True
        
    return False

def parse_sections(text: str):
    """
    è§£æ Markdown æ–‡æ¡£ä¸­çš„ç« èŠ‚ç»“æ„ï¼ˆæ”¯æŒ # 5.3.1 FRæ¥å…¥ æ ¼å¼ï¼‰
    """
    lines = text.splitlines()
    sections = []
    id_to_title = {}

    # âœ… æ”¯æŒ # å¼€å¤´ + æ•°å­—ç¼–å· + ç©ºæ ¼ + æ ‡é¢˜
    section_re = re.compile(r'^\s*#\s*(\d+(?:\.\d+){0,5})\s+(.+?)\s*$')

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # è·³è¿‡å¹²æ‰°é¡¹
        if re.match(r'^(è¡¨|ç¤ºä¾‹|æ³¨|å›¾|é™„å½•)\d+', line):
            continue

        m = section_re.match(line)
        if m:
            sec_id = m.group(1)
            title = m.group(2).strip()
            sections.append({"id": sec_id, "title": title})
            id_to_title[sec_id] = title

    # æ„å»ºçˆ¶å­å…³ç³»
    section_rels = []
    for sec in sections:
        sec_id = sec["id"]
        if '.' in sec_id:
            parent_id = '.'.join(sec_id.split('.')[:-1])
            if parent_id in id_to_title:
                section_rels.append({"parent": parent_id, "child": sec_id})

    return sections, section_rels

def extract_flag_lines(text: str) -> List[Dict]:
    rows=[]
    for raw in text.splitlines():
        line = raw.strip()
        m = FLAG_LINE.match(line)
        if not m: 
            continue
        flag  = (m.group("flag") or "").strip()
        alias = (m.group("alias") or "").strip(" ã€ï¼Œ, ")
        body  = (m.group("body")  or "").strip()
        if not flag or not body:
            continue
        s = f"{flag} æ ‡å¿—"
        rows.append({"s": s, "p_raw": "ç”¨é€”", "o": body, "snippet": line})
        if alias:
            rows.append({"s": s, "p_raw": "åˆ«å", "o": alias, "snippet": line})
    return rows

SECTION_ONLY = re.compile(r'^\s*(\d+(?:\.\d+){1,6}|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+|[A-Za-z])([.)ã€ï¼‰])?\s*$')
def clean(s:str)->str:
    if not s: return ""
    s = re.sub(r'\$[^$]*\$', ' ', s)          # å» LaTeX å…¬å¼
    s = re.sub(r'\s+', ' ', s).strip(' ï¼š:;ï¼Œ,ã€‚')
    return s

def drop_noise(x:str)->bool:
    if not x: return True
    if SECTION_ONLY.match(x): return True      # çº¯æ®µè½ç¼–å·
    if len(x) <= 1: return True                # å•å­—ç¬¦/å™ªå£°
    return False

FEWSHOT = r"""
ä»æŠ€æœ¯è§„èŒƒä¸­æ–‡æ®µè½ä¸­æŠ½å–äº‹å®ä¸‰å…ƒç»„ï¼Œä¿æŒä¸­æ–‡è°“è¯ï¼Œä¸åšåŒä¹‰å½’ä¸€ã€‚å¿½ç•¥çº¯ç¼–å·/å°èŠ‚æ ‡è®°ã€‚ä¸»è¯­/å®¾è¯­å°½é‡æ˜¯åè¯çŸ­è¯­ã€‚
è¾“å‡ºJSON: {"triples":[{"s":"ä¸»è¯­","p":"è°“è¯","o":"å®¾è¯­","evidence":"åŸå¥"}...]}

âš ï¸ å…³é”®æŒ‡ä»¤ï¼šä¸»è¯­å¿…é¡»æ˜¯å®ä½“åç§°ï¼ˆå¦‚â€œRS232æ¥å£â€ï¼‰ï¼Œè°“è¯å¿…é¡»æ˜¯æŠ€æœ¯å‚æ•°ï¼ˆå¦‚â€œé€šä¿¡åè®®â€ã€â€œæ¥å£é€Ÿç‡â€ï¼‰ï¼Œå°½é‡ä¸è¦ä½¿ç”¨â€œåº”æ»¡è¶³â€ç­‰åŠ¨è¯æ€§è°“è¯ã€‚

ç¤ºä¾‹1ï¼ˆRS-232æ¥å£ï¼‰ï¼š
æ–‡æœ¬ï¼š
â€œRS232æ¥å…¥çš„è¦æ±‚å¦‚ä¸‹ï¼šâ€”â€”é€šä¿¡åè®®ï¼šå¼‚æ­¥é€šä¿¡åè®®ï¼Œç¬¦åˆITU-T X.24ï¼›â€”â€”æ¥å£é€Ÿç‡ï¼š50 bit/sï¼Œ75 bit/sï¼Œ100 bit/sï¼Œ300 bit/sï¼Œ600 bit/sï¼Œ1200 bit/sï¼Œ2400 bit/sï¼Œ4800 bit/sï¼Œ9600 bit/sï¼Œ19200 bit/sï¼›â€”â€”ä¼ è¾“ç ï¼šIA-5 ç åˆ¶æ—¶ï¼ŒåŒ…å«7æˆ–8ä¸ªæ•°æ®ä½ã€1æˆ–2ä¸ªåœæ­¢ä½ã€æ— æ ¡éªŒä½ï¼›ä¼ è¾“ç ä¸ºITA-2 ç åˆ¶æ—¶ï¼ŒåŒ…å«5ä¸ªæ•°æ®ä½ã€1.5ä¸ªåœæ­¢ä½ã€æ— æ ¡éªŒä½ã€‚â€
æœŸæœ›triplesï¼š
- RS-232 æ¥å£ â€”é€šä¿¡åè®®â†’ å¼‚æ­¥é€šä¿¡åè®®
- RS-232 æ¥å£ â€”ç¬¦åˆâ†’ ITU-T X.24
- RS-232 æ¥å£ â€”æ¥å£é€Ÿç‡â†’ 50 bit/sï¼Œ75 bit/sï¼Œ100 bit/sï¼Œ300 bit/sï¼Œ600 bit/sï¼Œ1200 bit/sï¼Œ2400 bit/sï¼Œ4800 bit/sï¼Œ9600 bit/sï¼Œ19200 bit/s
- IA-5 ç åˆ¶ â€”ä¼ è¾“ç â†’ 7æˆ–8ä¸ªæ•°æ®ä½ã€1æˆ–2ä¸ªåœæ­¢ä½ã€æ— æ ¡éªŒä½
- ITA-2 ç åˆ¶ â€”ä¼ è¾“ç â†’ 5ä¸ªæ•°æ®ä½ã€1.5ä¸ªåœæ­¢ä½ã€æ— æ ¡éªŒä½

ç¤ºä¾‹2ï¼ˆç”µæµç¯ï¼‰ï¼š
æ–‡æœ¬ï¼š
â€œç”µæµç¯æ¥å…¥â€¦â€¦ç”µå‹ä¸ºç›´æµÂ±24Vï¼Œç”µæµä¸ºç›´æµ4mAÂ±2mAï¼›æ¥å£é€Ÿç‡ï¼š50 bit/sï¼Œ100 bit/sï¼Œ300 bit/sï¼Œ600 bit/sï¼Œ1200 bit/sã€‚â€
æœŸæœ›ï¼š
- ç”µæµç¯æ¥å…¥ â€”ç”µå‹â†’ Â±24V ç›´æµ
- ç”µæµç¯æ¥å…¥ â€”ç”µæµâ†’ 4mAÂ±2mA ç›´æµ
- ç”µæµç¯æ¥å…¥ â€”æ¥å£é€Ÿç‡â†’ 50 bit/sï¼Œ100 bit/sï¼Œ300 bit/sï¼Œ600 bit/sï¼Œ1200 bit/s

ç¤ºä¾‹3ï¼ˆåœ°å€é•¿åº¦ä¸ç»„æˆï¼‰ï¼š
æ–‡æœ¬ï¼š
â€œæ¯ä¸ªSITAæ”¶ç”µåœ°å€ç”±7ä¸ªå­—ç¬¦ç»„æˆï¼šå‰3ä½åŸå¸‚æˆ–æœºåœºä»£ç ï¼Œä¸­2ä½éƒ¨é—¨ä»£ç ï¼Œå2ä½ç½‘ç»œç”¨æˆ·ä»£ç ï¼ˆIATAå®šä¹‰ï¼‰ã€‚â€
æœŸæœ›ï¼š
- SITA æ”¶ç”µåœ°å€ â€”é•¿åº¦â†’ 7å­—ç¬¦
- SITA æ”¶ç”µåœ°å€ â€”ç»„æˆâ†’ åŸå¸‚æˆ–æœºåœºä»£ç (3ä½)
- SITA æ”¶ç”µåœ°å€ â€”ç»„æˆâ†’ éƒ¨é—¨ä»£ç (2ä½)
- SITA æ”¶ç”µåœ°å€ â€”ç»„æˆâ†’ ç½‘ç»œç”¨æˆ·ä»£ç (2ä½)

ç¤ºä¾‹4ï¼ˆAFTNåœ°å€ï¼‰ï¼š
æ–‡æœ¬ï¼š
â€œæ”¶ç”µåœ°å€æ ‡è¯†ä¸º8ä½å­—ç¬¦ï¼šç¬¬1-2ä½å›½å®¶æƒ…æŠ¥åŒºï¼›ç¬¬3-4ä½é€šä¿¡ä¸­å¿ƒï¼›ç¬¬5-7ä½A-Zï¼›ç¬¬8ä½ç»„ç»‡ç»†åˆ†æˆ–Xå¡«å……ã€‚â€
æœŸæœ›ï¼š
- AFTN æ”¶ç”µåœ°å€ â€”é•¿åº¦â†’ 8å­—ç¬¦
- AFTN æ”¶ç”µåœ°å€ â€”ç»„æˆâ†’ å›½å®¶æƒ…æŠ¥åŒº(2ä½)
- AFTN æ”¶ç”µåœ°å€ â€”ç»„æˆâ†’ é€šä¿¡ä¸­å¿ƒ(2ä½)
- AFTN æ”¶ç”µåœ°å€ â€”ç»„æˆâ†’ ç»„ç»‡ç»†åˆ†(ç¬¬8ä½)

ç¤ºä¾‹5ï¼ˆä»¥å¤ªç½‘ï¼‰ï¼š
æ–‡æœ¬ï¼š
â€œä»¥å¤ªç½‘æ¥å…¥â€¦â€¦é€šä¿¡åè®®ï¼šç¬¦åˆIEEE802.3ï¼›ç½‘ç»œåè®®ï¼šIPX/SPXã€TCP/IPã€UDP/IPï¼›æ¥å£é€Ÿç‡ï¼š10 Mbit/sï¼Œ100 Mbit/sï¼Œ1000 Mbit/sã€‚â€
æœŸæœ›ï¼š
- ä»¥å¤ªç½‘æ¥å£ â€”é€šä¿¡åè®®â†’ IEEE 802.3
- ä»¥å¤ªç½‘æ¥å£ â€”ç½‘ç»œåè®®â†’ IPX/SPXã€TCP/IPã€UDP/IP
- ä»¥å¤ªç½‘æ¥å£ â€”æ¥å£é€Ÿç‡â†’ 10 Mbit/sï¼Œ100 Mbit/sï¼Œ1000 Mbit/s

ç¤ºä¾‹6ï¼ˆç¯å¢ƒèŒƒå›´ï¼‰ï¼š
æ–‡æœ¬ï¼š
â€œå·¥ä½œæ¸©åº¦ï¼š0Â°C~40Â°Cï¼›ç›¸å¯¹æ¹¿åº¦ï¼š20%~80%ï¼›è®¾å¤‡ä¾›ç”µï¼šç”µå‹220VÂ±20Vï¼Œé¢‘ç‡50Hzï¼›æç«¯æµ·æ‹”ä¸è¶…è¿‡5000mã€‚â€
æœŸæœ›ï¼š
- å·¥ä½œæ¸©åº¦ â€”èŒƒå›´â†’ 0Â°C~40Â°C
- ç›¸å¯¹æ¹¿åº¦ â€”èŒƒå›´â†’ 20%~80%
- ä¾›ç”µ â€”ç”µå‹â†’ 220VÂ±20V
- ä¾›ç”µ â€”é¢‘ç‡â†’ 50Hz
- ç¯å¢ƒ â€”æµ·æ‹”ä¸Šé™â†’ 5000m

ç¤ºä¾‹7ï¼ˆæ ‡å¿—è¡Œï¼‰ï¼š
æ–‡æœ¬ï¼š
â€œâ€”â€”COLï¼ˆæ ¡å¯¹ã€æ ¸å¯¹ï¼‰ï¼šåœ¨æ–°çš„ç”µæŠ¥ä¸­å¯¹åŸæ¥é‡è¦çš„ç”µæŠ¥è¿›è¡Œæ ¡å¯¹æ—¶ï¼Œåœ¨æ ¡å¯¹å‰¯æœ¬ä¹‹å‰åº”ä½¿ç”¨COLæ ‡å¿—ï¼›â€
æœŸæœ›ï¼š
- COL æ ‡å¿— â€”ç”¨é€”â†’ åœ¨æ–°çš„ç”µæŠ¥ä¸­å¯¹åŸæ¥é‡è¦çš„ç”µæŠ¥è¿›è¡Œæ ¡å¯¹æ—¶ï¼Œåœ¨æ ¡å¯¹å‰¯æœ¬ä¹‹å‰åº”ä½¿ç”¨COLæ ‡å¿—
- COL æ ‡å¿— â€”åˆ«åâ†’ æ ¡å¯¹ã€æ ¸å¯¹
"""

def call_llm(text: str, client: OpenAI, model: str) -> List[Dict]:
    prompt = FEWSHOT + "\nå¾…æŠ½å–æ–‡æœ¬ï¼š\n" + text[:3500]
    msgs = [
        {"role":"system","content":"ä¸¥æ ¼è¿”å›JSONï¼Œä¸è¦è§£é‡Šã€‚"},
        {"role":"user","content": prompt}
    ]
    for _ in range(3):
        try:
            rsp = client.chat.completions.create(
                model=model, temperature=0.1, max_tokens=1400,
                response_format={"type":"json_object"}, messages=msgs
            )
            data = json.loads(rsp.choices[0].message.content or "{}")
            triples = data.get("triples", [])
            out=[]
            for t in triples:
                s = clean(t.get("s","")); p = clean(t.get("p","")); o = clean(t.get("o",""))
                ev = clean(t.get("evidence",""))
                if drop_noise(s) or not p or not o: continue
                if re.match(r'^\d+(?:\.\d+){1,5}$', s.strip()):
                    continue
                out.append({"s":s, "p_raw":p, "o":o, "snippet":ev})
            return out
        except Exception:
            time.sleep(1.2)
    return []

def chunk_with_section(text: str, size=1400, overlap=180):
    lines = text.splitlines()
    chunks = []
    current_section = ""
    buffer = ""
    section_re = re.compile(r'^\s*#\s*(\d+(?:\.\d+){0,5})\s+(.+?)\s*$')

    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue

        m = section_re.match(stripped)
        if m:
            # ä¿å­˜ä¸Šä¸€ä¸ª chunkï¼ˆå¦‚æœæœ‰ï¼‰
            if buffer.strip():
                chunks.append((buffer.strip(), current_section))
            # å¼€å§‹æ–° chunkï¼šå°†ç« èŠ‚è¡ŒåŠ å…¥ buffer
            current_section = m.group(1)
            buffer = line  # ğŸ‘ˆ å…³é”®ï¼šä¿ç•™ç« èŠ‚è¡Œ
            continue

        if len(buffer) + len(line) <= size:
            buffer += "\n" + line
        else:
            if buffer.strip():
                chunks.append((buffer.strip(), current_section))
            buffer = line[-overlap:] + "\n" + line

    if buffer.strip():
        chunks.append((buffer.strip(), current_section))

    return chunks

def push_enhanced(llm_triples, section_nodes, section_rels, uri, user, pwd, src):
    if not llm_triples and not section_nodes:
        return

    drv = GraphDatabase.driver(uri, auth=(user, pwd))
    with drv.session() as session:
        # åˆ›å»ºå”¯ä¸€çº¦æŸ
        session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE")
        session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (s:Section) REQUIRE s.id IS UNIQUE")

        # 1. å…¥åº“ LLM ä¸‰å…ƒç»„ï¼ˆä¿æŒä½ åŸæœ‰çš„é€»è¾‘ï¼‰
        if llm_triples:
            for r in llm_triples:
                r["src"] = src
            session.run("""
            UNWIND $rows AS r
            MERGE (s:Entity {name: r.s})
            MERGE (o:Entity {name: r.o})
            MERGE (s)-[e:REL {p_raw: r.p_raw}]->(o)
            SET e.type = 'FACT',
                e.source = r.src,
                e.snippet = r.snippet
            """, rows=llm_triples)

        # 2. å…¥åº“ Section èŠ‚ç‚¹
        if section_nodes:
            session.run("""
            UNWIND $rows AS r
            MERGE (s:Section {id: r.id})
            SET s.title = r.title,
                s.cleanTitle = r.clean_title,
                s.source = $src
            """, rows=section_nodes, src=src)

        # 3. å…¥åº“ Section å±‚çº§å…³ç³»
        if section_rels:
            session.run("""
            UNWIND $rows AS r
            MATCH (p:Section {id: r.parent})
            MATCH (c:Section {id: r.child})
            MERGE (p)-[:HAS_SUBSECTION]->(c)
            """, rows=section_rels)
        
        # âœ… 4. æ–°å¢ï¼šå»ºç«‹ Entity -> Section çš„ DEFINED_IN å…³ç³»
        defined_in_triples = [
            {"entity": r["s"], "section_id": r.get("section_id", "")}
            for r in llm_triples
            if r.get("section_id")
        ]
        if defined_in_triples:
            session.run("""
            UNWIND $rows AS r
            MATCH (e:Entity {name: r.entity})
            MATCH (s:Section {id: r.section_id})
            MERGE (e)-[:DEFINED_IN]->(s)
            """, rows=defined_in_triples)

        # âœ…âœ…âœ… 5. æ–°å¢ï¼šè‡ªåŠ¨åŒ–åˆ›å»ºæœ‰æ„ä¹‰çš„ç« èŠ‚EntityèŠ‚ç‚¹ + åå‘INCLUDESå…³ç³»
        if section_nodes and llm_triples:
            # æ„å»ºç« èŠ‚IDåˆ°å­ç« èŠ‚æ•°é‡çš„æ˜ å°„
            child_count = defaultdict(int)
            for rel in section_rels:
                child_count[rel["parent"]] += 1
            
            # æ”¶é›†æ‰€æœ‰section_idåˆ°æ ‡é¢˜çš„æ˜ å°„
            id_to_title = {s["id"]: s["title"] for s in section_nodes}
            
            # åˆ¤æ–­å“ªäº›ç« èŠ‚éœ€è¦åˆ›å»ºEntityèŠ‚ç‚¹
            meaningful_entities = []
            for sec in section_nodes:
                sec_id = sec["id"]
                title = sec["title"]
                count = child_count[sec_id]
                
                if should_create_entity_node(sec_id, title, count):
                    meaningful_entities.append({"name": title})
            
            # å…¥åº“æœ‰æ„ä¹‰çš„EntityèŠ‚ç‚¹
            if meaningful_entities:
                session.run("""
                UNWIND $rows AS r
                MERGE (e:Entity {name: r.name})
                SET e.type = 'CATEGORY', e.source = $src
                """, rows=meaningful_entities, src=src)
            
            # å»ºç«‹åå‘INCLUDESå…³ç³»ï¼ˆç« èŠ‚Entity â†’ æŠ€æœ¯å®ä½“ï¼‰
            if meaningful_entities:
                # æ„å»ºæ ‡é¢˜åˆ°æ˜¯å¦å­˜åœ¨Entityçš„æ˜ å°„
                title_to_entity = {e["name"] for e in meaningful_entities}
                
                includes_rels = []
                for r in llm_triples:
                    if r.get("section_id"):
                        sec_id = r["section_id"]
                        # å‘ä¸Šéå†æ‰€æœ‰ç¥–å…ˆç« èŠ‚
                        current_id = sec_id
                        while current_id in id_to_title:
                            title = id_to_title[current_id]
                            if title in title_to_entity:
                                includes_rels.append({
                                    "category": title,
                                    "entity": r["s"],
                                    "source": r["src"]
                                })
                            if '.' in current_id:
                                current_id = '.'.join(current_id.split('.')[:-1])
                            else:
                                break
                
                # å»é‡ï¼ˆé¿å…åŒä¸€å®ä½“è¢«å¤šæ¬¡å…³è”ï¼‰
                unique_includes = []
                seen = set()
                for rel in includes_rels:
                    key = (rel["category"], rel["entity"])
                    if key not in seen:
                        seen.add(key)
                        unique_includes.append(rel)
                
                if unique_includes:
                    session.run("""
                    UNWIND $rows AS r
                    MATCH (cat:Entity {name: r.category})
                    MATCH (e:Entity {name: r.entity})
                    MERGE (cat)-[rel:INCLUDES]->(e)
                    SET rel.source = r.source,
                        rel.type = 'CATEGORY_REL'
                    """, rows=unique_includes)

    drv.close()

def run(pattern: str, uri: str, user: str, pwd: str, base: str, key: str, model: str):
    client = OpenAI(base_url=base, api_key=key)
    files = sorted(glob.glob(pattern))
    
    # å®šä¹‰ç« èŠ‚æ­£åˆ™ï¼ˆä¸ parse_sections ä¸€è‡´ï¼‰
    section_re = re.compile(r'^\s*#\s*(\d+(?:\.\d+){0,5})\s+(.+?)\s*$')

    for fp in files:
        with open(fp, "r", encoding="utf-8", errors="ignore") as f:
            txt = f.read()

        # === 1. è§„åˆ™æŠ½å–ï¼ˆæ ‡å¿—è¡Œï¼‰===
        rule_rows = extract_flag_lines(txt)
        all_rows, seen = [], set()
        for t in rule_rows:
            sig = (t["s"], t["p_raw"], t["o"])
            if sig in seen: 
                continue
            seen.add(sig)
            # ä¸ºè§„åˆ™è¡Œä¹Ÿå°è¯•åˆ†é…ç« èŠ‚ï¼ˆå¯é€‰ï¼‰
            t["section_id"] = ""  # æš‚ä¸å¤„ç†ï¼Œç®€å•èµ·è§
            all_rows.append(t)

        # === 2. LLM æŠ½å–ï¼ˆæŒ‰ç« èŠ‚ç²¾ç¡®åˆ†å—ï¼‰===
        lines = txt.splitlines()
        chunks_with_sec = []
        current_section = ""
        current_content_lines = []

        section_re = re.compile(r'^\s*#\s*(\d+(?:\.\d+){0,5})\s+(.+?)\s*$')

        for line in lines:
            stripped = line.strip()
            if not stripped:
                continue

            # æ£€æŸ¥æ˜¯å¦ä¸ºç« èŠ‚è¡Œ
            m = section_re.match(stripped)
            if m:
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if current_section and current_content_lines:
                    chunk_text = "\n".join(current_content_lines)
                    chunks_with_sec.append((chunk_text, current_section))
                    current_content_lines = []

                # å¼€å§‹æ–°ç« èŠ‚
                current_section = m.group(1)
                current_content_lines.append(line)  # ä¿ç•™ç« èŠ‚è¡Œæœ¬èº«
                continue

            # ç´¯ç§¯éç« èŠ‚è¡Œåˆ°å½“å‰ç« èŠ‚
            if current_section:
                current_content_lines.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section and current_content_lines:
            chunk_text = "\n".join(current_content_lines)
            chunks_with_sec.append((chunk_text, current_section))

        # å¤„ç†æ¯ä¸ªå—
        t_file0 = time.time()
        with tqdm(total=len(chunks_with_sec), desc=f"Extract {os.path.basename(fp)}", unit="chunk") as bar:
            avg, n = 0.0, 0
            for ck, sec_id in chunks_with_sec:
                t0 = time.time()
                triples = call_llm(ck, client, model)
                dt = time.time() - t0
                n += 1
                avg = (avg * (n - 1) + dt) / n
                bar.set_postfix(last_s=f"{dt:.2f}", avg_s=f"{avg:.2f}", triples=len(triples))
                bar.update(1)

                for t in triples:
                    sig = (t["s"], t["p_raw"], t["o"])
                    if sig in seen:
                        continue
                    seen.add(sig)
                    t["section_id"] = sec_id  # ğŸ‘ˆ å…³é”®ï¼šé™„åŠ ç« èŠ‚ID
                    all_rows.append(t)

        # === 3. è§£æç« èŠ‚ç»“æ„ ===
        section_nodes, section_rels = parse_sections(txt)

        # === 4. å…¥åº“ï¼šå®ä½“ + ç« èŠ‚ + å…³è” ===
        push_enhanced(
            llm_triples=all_rows,
            section_nodes=section_nodes,
            section_rels=section_rels,
            uri=uri, user=user, pwd=pwd, src=fp
        )

        print(f"{os.path.basename(fp)} -> "
              f"{len(all_rows)} facts | "
              f"{len(section_nodes)} sections | "
              f"{time.time() - t_file0:.1f}s")


if __name__ == "__main__":
    import argparse
    ap=argparse.ArgumentParser()
    ap.add_argument("--glob", default="/home/mxq/Develop/kg_data/md/*.md")
    ap.add_argument("--uri", default="bolt://localhost:7687")
    ap.add_argument("--user", default="neo4j")
    ap.add_argument("--password", default="neo4j_pass123")
    ap.add_argument("--api_base", default=os.getenv("OPENAI_BASE_URL","http://localhost:11434/v1"))
    ap.add_argument("--api_key",  default=os.getenv("OPENAI_API_KEY","sk-xxx"))
    ap.add_argument("--model",    default=os.getenv("OPENAI_MODEL","qwen2.5-14b-instruct"))
    args=ap.parse_args()
    run(args.glob, args.uri, args.user, args.password, args.api_base, args.api_key, args.model)